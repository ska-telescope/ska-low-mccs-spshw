display: ":0"
xauthority: "~/.Xauthority"

enabled: true

system: SW-infrastructure
subsystem: low-mccs-spshw 
telescope: SKA-low

labels:
  app: ska-low-mccs-spshw

global:
  sub-system:
    ska-tango-base:
      enabled: false
    taranta:
      enabled: false
    archiver:
      enabled: false
  minikube: true
  tango_host: databaseds-tango-base-test:10000

livenessProbe:
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 3
  successThreshold: 1
  failureThreshold: 3
readinessProbe:
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 3
  successThreshold: 1
  failureThreshold: 3

ska-taranta:
  enabled: true
  ingress:
    enabled: true
    nginx: true
    hostname: k8s.stfc.skao.int
  tangogql:
    replicas: 1
    resources:
      requests:
        cpu: 600m  # 600m = 0.6 CPU
        memory: 512Mi  # 512Mi = 0.5 GB mem

ska-taranta-dashboard:
  ingress:
    enabled: true
    nginx: true
    hostname: test-taranta.k8s.stfc.skao.int

ska-taranta-auth:
  ingress:
    enabled: true
    nginx: true
    hostname: test-taranta.k8s.stfc.skao.int

# Jupyterhub is a large image and you may experience a kubelet timeout MCCS-1238.
# Error: UPGRADE FAILED: pre-upgrade hooks failed: timed out waiting for the condition
# You can perform a manual pull using: 
# minikube image pull registry.gitlab.com/ska-telescope/sdi/ska-cicd-deploy-low-itf/tango-notebook:0.1.1
# Jupyter has ingress meaning you can access using ${MINIKUBE_IP}/jupyterhub/
# e.g 192.168.49.2/jupyterhub/
jupyterhub:
  hub:
    baseUrl: /jupyterhub
  ingress:
    pathType: Prefix
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
  singleuser:
    image:
      name: registry.gitlab.com/ska-telescope/sdi/ska-cicd-deploy-low-itf/tango-notebook
      tag: 0.1.1
    extraEnv:
      TANGO_HOST: "databaseds-tango-base-test.ska-low-mccs-spshw:10000"
  proxy:
    service:
      type: ClusterIP

# SKUID 
ska-ser-skuid:
  enabled: true
  skuid:
    use_pv: false
    config:
      generator_id: "mvp01"
 
# The following values are used by the templates in this chart for create pgAdmin and initialising the database
pgadmin4:
  enabled: true

  replicaCount: 1

  service:
    type: LoadBalancer
    port: 80

  image:
    registry: docker.io
    repository: dpage/pgadmin4
    tag: "6.10"
    pullPolicy: IfNotPresent

  env:
    # can be email or nickname
    email: mccs@skao.int
    password: secretpassword

  serverDefinitions:
    ## If true, server definitions will be created
    ##
    enabled: true

    servers:
     firstServer:
       Name: "MCCS"
       Group: "Servers"
       Port: 5432
       Username: "postgres"
       Host: "test-postgresql"
       SSLMode: "prefer"
       MaintenanceDB: "postgres"

  resources: {}

  containerPorts:
    http: 80

  ingress:
    enabled: true

# ska-low-mccs-daq:
#   receivers: # DAQ Server. (backend)
#     1: {}
      # gpu_limit: 1
      # runtimeclassname: nvidia

ska-low-mccs-spshw:
  simulators:
    subracks:
      1:
        host: subrack-simulator-1
  receivers:
    1: {}
      # external_ip: # optional: if provided, this is the IP address of an external DAQ receiver
      # port:  # optional: defaults to 50051
      # storage: daq-data  # must refer to a PVC: pre-existing or created above
      # server_node_selector: {}  # optional
      # server_annotations: {}  # optional
      # server_affinity: {}  # optional
      # runtimeclassname: nvidia # optional: must refer to a RunTimeClass existing in the cluster.
      # gpu_limit: "1" # optional

  deviceServers:  # e.g....
    stations:
      instances:
        1:
          cabinet_network_address: 10.0.0.0
          subracks: [1]
          tpms: [1]
    stationcalibrators:
      instances:
        1:
          field_station: 1
          calibration_store: 1
    calibrationstores:
      instances:
        1:
          logging_level: 5
    mockfieldstations:
      instances:
        1:
          logging_level: 5
    tpms:
      instances:
        1:
          host: 10.0.10.201
          port: 10000
          version: tpm_v1_6
          subrack: "1"
          subrack_bay: 1
      logging_level_default: 5
      simulation_config: 1
      test_config: 1
      resources:
        requests:
          memory: 512Mi

  receivers: # DAQ Tango device. (frontend)
    1:
      host: "daq-receiver-001"  # optional: hostname of a pre-existing DAQ
      # port: 50051  # optional: defaults to 50051
      # receiver_interface: net1  # optional: defaults to "eth0"
      # receiver_ip: ""  # empty = Determine from interface name
      # receiver_port: 5660  # optional: defaults to 4660
      # logging_level_default: 5
